{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6648a7d-da45-4d68-8a9f-b5899319059b",
   "metadata": {},
   "source": [
    "<font size=\"5\"><b><center>TEXT SUMMARIZATION</center></b></font>\n",
    "<font size=\"2\"><b><center>ASSIGNMENT</center></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ac0ef-7d51-45f5-a27a-8938ced95463",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b><CENTER>TEXT1 SUMMARIZATION</CENTER></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa239ac-ad0c-4c26-bb2a-7aa81e6ca09e",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>IMPORTING NEEDED MODELS</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dbb7f01-1140-4956-9ac0-ecb8fb9a7da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: networkx in c:\\users\\aleky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install networkx\n",
    "\n",
    "from nltk.corpus import stopwords #you can remove stop words for speed\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16849e8b-7cbe-41a6-a904-73a84503e469",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>LOADING DATA SET </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18a53eb4-aa98-4b5f-b285-b4c297d5a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gallery unveils interactive tree\n",
      "\n",
      "A Christmas tree that can receive text messages has been unveiled at London's Tate Britain art gallery.\n",
      "\n",
      "The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate. The messages will be \"unwrapped\" by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs. It is the 17th year that the gallery has invited an artist to dress their Christmas tree. Artists who have decorated the Tate tree in previous years include Tracey Emin in 2002.\n",
      "\n",
      "The plain green Norway spruce is displayed in the gallery's foyer. Its light bulb adornments are dimmed, ordinary domestic ones joined together with string. The plates decorating the branches will be auctioned off for the children's charity ArtWorks. Wentworth worked as an assistant to sculptor Henry Moore in the late 1960s. His reputation as a sculptor grew in the 1980s, while he has been one of the most influential teachers during the last two decades. Wentworth is also known for his photography of mundane, everyday subjects such as a cigarette packet jammed under the wonky leg of a table.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text1.1.txt', 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "    contents = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d4f03f-6242-4401-843d-3c1eef1e9788",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OPEN FILE AND SPLIT INTO SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1e90fd83-3d75-40e4-adcb-b39feb0e7895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate\n",
      "The messages will be \"unwrapped\" by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs\n",
      "It is the 17th year that the gallery has invited an artist to dress their Christmas tree\n",
      "Artists who have decorated the Tate tree in previous years include Tracey Emin in 2002.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the file \"Text1.txt\" in read mode\n",
    "file = open(\"text1.1.txt\", \"r\")\n",
    "\n",
    "# Read all lines of the file into a list\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Split the first paragraph into sentences based on the period followed by a space\n",
    "article = filedata[4].split(\". \")  # Assuming each sentence ends with \". \".\n",
    "\n",
    "# Initialize an empty list to store individual sentences\n",
    "sentences = []\n",
    "\n",
    "# Iterate through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Print each sentence\n",
    "    print(sentence)\n",
    "    # Replace non-alphabetic characters with spaces and split the sentence into words\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10810b83-a081-490f-8e1e-2c9868dcd1cc",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OUR DATA: A LIST OF SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "43387370-8aaa-4ddf-ab2b-4956b3d70083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['The', 'spruce', 'has', 'an', 'antenna', 'which', 'can', 'receive', 'Bluetooth', 'texts', 'sent', 'by', 'visitors', 'to', 'the', 'Tate'], ['The', 'messages', 'will', 'be', '\"unwrapped\"', 'by', 'sculptor', 'Richard', 'Wentworth,', 'who', 'is', 'responsible', 'for', 'decorating', 'the', 'tree', 'with', 'broken', 'plates', 'and', 'light', 'bulbs'], ['It', 'is', 'the', '17th', 'year', 'that', 'the', 'gallery', 'has', 'invited', 'an', 'artist', 'to', 'dress', 'their', 'Christmas', 'tree'], ['Artists', 'who', 'have', 'decorated', 'the', 'Tate', 'tree', 'in', 'previous', 'years', 'include', 'Tracey', 'Emin', 'in', '2002.\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617790d0-1517-40da-b2f3-a9dadb04ca31",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>FUNCTION TO CALCULATE SIMILARITY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "439bea5d-7fed-4e0c-82b7-0142cf361605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    # Convert all words to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    # Get all unique words from both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Initialize vectors for both sentences with zeros\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        return 1 - cosine_distance(vector1, vector2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d8873-21f3-46f4-a68f-ae1dff83adad",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>CREATING THE SIMILARITY MATRIX</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64285255-2a43-42a6-9b18-50c780dfee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.47140452 0.         0.        ]\n",
      " [0.40824829 0.         0.         0.        ]\n",
      " [0.45883147 0.45883147 0.         0.        ]\n",
      " [0.24253563 0.24253563 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0908928-73d8-4300-ace1-bcc7c8f93cff",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>GET THE PAGERANK SCORES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "418b4816-5da0-495e-888e-acf023442ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank scores: {0: 0.30085799159631177, 1: 0.30085799159631177, 2: 0.24899107429829256, 3: 0.14929294250908376}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph from the similarity matrix using NetworkX\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores\n",
    "print(\"PageRank scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f5f62b-d209-4b0c-bbf1-b3a416cbc11a",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>SORT SENTENCES BY PAGERANK</b></font></b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3acf08e6-2ef7-41a4-9a59-89a70656ddb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked sentences in descending order:\n",
      " [(0.30085799159631177, ['The', 'spruce', 'has', 'an', 'antenna', 'which', 'can', 'receive', 'Bluetooth', 'texts', 'sent', 'by', 'visitors', 'to', 'the', 'Tate']), (0.30085799159631177, ['The', 'messages', 'will', 'be', '\"unwrapped\"', 'by', 'sculptor', 'Richard', 'Wentworth,', 'who', 'is', 'responsible', 'for', 'decorating', 'the', 'tree', 'with', 'broken', 'plates', 'and', 'light', 'bulbs']), (0.24899107429829256, ['It', 'is', 'the', '17th', 'year', 'that', 'the', 'gallery', 'has', 'invited', 'an', 'artist', 'to', 'dress', 'their', 'Christmas', 'tree']), (0.14929294250908376, ['Artists', 'who', 'have', 'decorated', 'the', 'Tate', 'tree', 'in', 'previous', 'years', 'include', 'Tracey', 'Emin', 'in', '2002.\\n'])]\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes of top-ranked sentences in the order of their importance\n",
    "print(\"Indexes of top ranked sentences in descending order:\\n\", ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abf0b83-8309-4c55-975d-f02ff2f5fdc2",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PICK THE TOP \"N\" SENTENCES </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25a33e83-fc2f-4599-9844-e1bc2b4cd428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  2\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate through the top n ranked sentences and join them into a single string\n",
    "for i in range(n):\n",
    "    # Append the joined sentence to the summarize_text list\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410fbb1-1c0d-46c9-96ad-fc120b353529",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PRINTING THE SUMMARY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ff2197fb-ca99-4bcb-b1db-7ae26327ac41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      " The spruce has an antenna which can receive Bluetooth texts sent by visitors to the Tate. The messages will be \"unwrapped\" by sculptor Richard Wentworth, who is responsible for decorating the tree with broken plates and light bulbs\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Output the summarized text\n",
    "\n",
    "# Print the summarized text by joining the sentences with a period and a space\n",
    "print(\"Summarized Text:\\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1523fa-ed23-4615-93f4-5ab3ba5ac4c6",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b><CENTER>TEXT2 SUMMARIZATION</CENTER></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3e25a-542b-4dad-95a4-fe080d630ed8",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>LOADING DATA SET </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1921f3b2-2a33-4d58-bffa-c15a81a1919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarre joins fairytale celebration\n",
      "\n",
      "French musician Jean-Michel Jarre is to perform at a concert in Copenhagen to mark the bicentennial of the birth of writer Hans Christian Andersen.\n",
      "\n",
      "Denmark is holding a three-day celebration of the life of the fairy-tale author, with a concert at Parken stadium on 2 April. Other stars are expected to join the line-up in the coming months, and the Danish royal family will attend. \"Christian Andersen's fairy tales are timeless and universal,\" said Jarre. \"For all of us, at any age there is always - beyond the pure enjoyment of the tale - a message to learn.\" There are year-long celebrations planned across the world to celebrate Andersen and his work, which includes The Emperor's New Clothes and The Little Mermaid. Denmark's Crown Prince Frederik and Crown Princess Mary visited New York on Monday to help promote the festivities. The pair were at a Manhattan library to honour US literary critic Harold Bloom \"the international icon we thought we knew so well\".\n",
      "\n",
      "\"Bloom recognizes the darker aspects of Andersen's authorship,\" Prince Frederik said. Bloom is to be formally presented with the Hans Christian Andersen Award this spring in Anderson's hometown of Odense. The royal couple also visited the Hans Christian Anderson School complex, where Queen Mary read The Ugly Duckling to the young audience. Later at a gala dinner, Danish supermodel Helena Christensen was named a Hans Christian Andersen ambassador. Other ambassadors include actors Harvey Keitel and Sir Roger Moore, athlete Cathy Freeman and Brazilian soccer legend Pele.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text1.2.txt', 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "    contents = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d32c5b-f8b9-4607-a3af-96d36a1f30a6",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OPEN FILE AND SPLIT INTO SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15accd17-bf3f-4bdd-8fe4-0aec39978cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jarre joins fairytale celebration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the file \"Text1.txt\" in read mode\n",
    "file = open(\"text1.2.txt\", \"r\")\n",
    "\n",
    "# Read all lines of the file into a list\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Split the first paragraph into sentences based on the period followed by a space\n",
    "article = filedata[0].split(\". \")  # Assuming each sentence ends with \". \".\n",
    "\n",
    "# Initialize an empty list to store individual sentences\n",
    "sentences = []\n",
    "\n",
    "# Iterate through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Print each sentence\n",
    "    print(sentence)\n",
    "    # Replace non-alphabetic characters with spaces and split the sentence into words\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c76255d-d562-4a44-9e6a-7e99b8a87861",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OUR DATA: A LIST OF SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "260e3aff-2da3-4fd6-a06b-ccf4fc295539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['Jarre', 'joins', 'fairytale', 'celebration\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c26534b-bfe6-4978-8fb0-32592ad58c61",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>FUNCTION TO CALCULATE SIMILARITY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d1e4dd4-518f-410f-b6e4-dd8a33b5388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    # Convert all words to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    # Get all unique words from both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Initialize vectors for both sentences with zeros\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f45b1-1e41-4af0-a3ec-27239a1e3375",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>CREATING THE SIMILARITY MATRIX</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8510def-fe75-40b9-b7b3-828842b5a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8750f5-9eb8-46ad-801a-055347357e72",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>GET THE PAGERANK SCORES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f922ae9-b3d1-4e0f-bfaa-1fc4b4e6fb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank scores: {0: 1.0}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph from the similarity matrix using NetworkX\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores\n",
    "print(\"PageRank scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc762d53-46b5-4599-b908-9b10e0f210ed",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>SORT SENTENCES BY PAGERANK</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d964353-12f8-493f-af49-abe39c82297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked sentences in descending order:\n",
      " [(1.0, ['Jarre', 'joins', 'fairytale', 'celebration\\n'])]\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes of top-ranked sentences in the order of their importance\n",
    "print(\"Indexes of top ranked sentences in descending order:\\n\", ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73ce8f-2bfe-4687-a4a9-558737cbedc4",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PICK THE TOP \"N\" SENTENCES </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89100e3b-312f-4228-b667-04da61676a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  1\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate through the top n ranked sentences and join them into a single string\n",
    "for i in range(n):\n",
    "    # Append the joined sentence to the summarize_text list\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e4eb6d-14a8-4167-9aa1-9fb1edb86f9a",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PRINTING THE SUMMARY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0562b50-d44a-44ea-b959-22292fde6b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      " Jarre joins fairytale celebration\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Output the summarized text\n",
    "\n",
    "# Print the summarized text by joining the sentences with a period and a space\n",
    "print(\"Summarized Text:\\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10892eb2-3a24-4a5f-acd8-eb941e5ff0e0",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b><CENTER>TEXT3 SUMMARIZATION</CENTER></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457a5658-eaaa-46a3-9b94-b95f10231494",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>LOADING DATA SET </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a49ee4d-5262-4e20-bcc2-417b2f2dfcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musical treatment for Capra film\n",
      "\n",
      "The classic film It's A Wonderful Life is to be turned into a musical by the producer of the controversial hit show Jerry Springer - The Opera.\n",
      "\n",
      "Frank Capra's 1946 movie starring James Stewart, is being turned into a Â£7m musical by producer Jon Thoday. He is working with Steve Brown, who wrote the award-winning musical Spend Spend Spend. A spokeswoman said the plans were in the \"very early stages\", with no cast, opening date or theatre announced.\n",
      "\n",
      "A series of workshops have been held in London, and on Wednesday a cast of singers unveiled the musical to a select group of potential investors. Mr Thoday said the idea of turning the film into a musical had been an ambition of his for almost 20 years. It's a Wonderful Life was based on a short story, The Greatest Gift, by Philip van Doren Stern. Mr Thoday managed to buy the rights to the story from Van Doren Stern's family in 1999, following Mr Brown's success with Spend Spend Spend. He later secured the film rights from Paramount, enabling them to use the title It's A Wonderful Life.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text1.3.txt', 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "    contents = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98189daf-9f76-4a6b-ac8b-33681ab156dc",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OPEN FILE AND SPLIT INTO SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "20b48160-51b7-4bf0-b7f5-5d7b92361522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank Capra's 1946 movie starring James Stewart, is being turned into a Â£7m musical by producer Jon Thoday\n",
      "He is working with Steve Brown, who wrote the award-winning musical Spend Spend Spend\n",
      "A spokeswoman said the plans were in the \"very early stages\", with no cast, opening date or theatre announced.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the file \"Text1.txt\" in read mode\n",
    "file = open(\"text1.3.txt\", \"r\")\n",
    "\n",
    "# Read all lines of the file into a list\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Split the first paragraph into sentences based on the period followed by a space\n",
    "article = filedata[4].split(\". \")  # Assuming each sentence ends with \". \".\n",
    "\n",
    "# Initialize an empty list to store individual sentences\n",
    "sentences = []\n",
    "\n",
    "# Iterate through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Print each sentence\n",
    "    print(sentence)\n",
    "    # Replace non-alphabetic characters with spaces and split the sentence into words\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2775a6d-e525-4117-b617-d1135b4d43b0",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OUR DATA: A LIST OF SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d85c2f0a-5ebf-433a-a233-884e2613568b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['Frank', \"Capra's\", '1946', 'movie', 'starring', 'James', 'Stewart,', 'is', 'being', 'turned', 'into', 'a', 'Â£7m', 'musical', 'by', 'producer', 'Jon', 'Thoday'], ['He', 'is', 'working', 'with', 'Steve', 'Brown,', 'who', 'wrote', 'the', 'award-winning', 'musical', 'Spend', 'Spend', 'Spend'], ['A', 'spokeswoman', 'said', 'the', 'plans', 'were', 'in', 'the', '\"very', 'early', 'stages\",', 'with', 'no', 'cast,', 'opening', 'date', 'or', 'theatre', 'announced.\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41106d6d-1e10-4349-b3ed-dd99c80e4aa5",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>FUNCTION TO CALCULATE SIMILARITY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "889c8794-d19b-4ae4-964f-1c1254be18b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    # Convert all words to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    # Get all unique words from both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Initialize vectors for both sentences with zeros\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8f6f68-53b9-445f-a491-91b4c7d81210",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>CREATING THE SIMILARITY MATRIX</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37ae830a-4ada-430e-a38c-234f90083bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.         0.23570226]\n",
      " [0.         0.         0.        ]\n",
      " [0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3328f2-cc40-49e4-8674-80cfde45e04c",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>GET THE PAGERANK SCORES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d3ddeea9-1c39-403a-9073-e1a05c6cd6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank scores: {0: 0.46511615458001787, 1: 0.06976769083996354, 2: 0.46511615458001787}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph from the similarity matrix using NetworkX\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores\n",
    "print(\"PageRank scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b6036-218b-4958-a75d-fd09472df94a",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>SORT SENTENCES BY PAGERANK</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "137655de-0e9f-4299-aa18-19f9b2ef72ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked sentences in descending order:\n",
      " [(0.46511615458001787, ['Frank', \"Capra's\", '1946', 'movie', 'starring', 'James', 'Stewart,', 'is', 'being', 'turned', 'into', 'a', 'Â£7m', 'musical', 'by', 'producer', 'Jon', 'Thoday']), (0.46511615458001787, ['A', 'spokeswoman', 'said', 'the', 'plans', 'were', 'in', 'the', '\"very', 'early', 'stages\",', 'with', 'no', 'cast,', 'opening', 'date', 'or', 'theatre', 'announced.\\n']), (0.06976769083996354, ['He', 'is', 'working', 'with', 'Steve', 'Brown,', 'who', 'wrote', 'the', 'award-winning', 'musical', 'Spend', 'Spend', 'Spend'])]\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes of top-ranked sentences in the order of their importance\n",
    "print(\"Indexes of top ranked sentences in descending order:\\n\", ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4111d602-ad43-426a-bbf8-81778da0bde0",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PICK THE TOP \"N\" SENTENCES </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5d823e94-eee9-4aa1-8b17-49748321d55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  2\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate through the top n ranked sentences and join them into a single string\n",
    "for i in range(n):\n",
    "    # Append the joined sentence to the summarize_text list\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90b12d8-bfe7-40f6-b97e-ccd768419805",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PRINTING THE SUMMARY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d81a6d70-aebb-4ceb-a4fc-d849ec801de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      " Frank Capra's 1946 movie starring James Stewart, is being turned into a Â£7m musical by producer Jon Thoday. A spokeswoman said the plans were in the \"very early stages\", with no cast, opening date or theatre announced.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Output the summarized text\n",
    "\n",
    "# Print the summarized text by joining the sentences with a period and a space\n",
    "print(\"Summarized Text:\\n\", \". \".join(summarize_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab7ed4-2d41-4259-a14e-93cd4b3b4423",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b><CENTER>TEXT4 SUMMARIZATION</CENTER></b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5a9a7-1d22-4340-84db-b26c7d6df836",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>LOADING DATA SET </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4d0f2ce-e0e5-4d71-9c13-5b6ae824558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Richard and Judy choose top books\n",
      "\n",
      "The 10 authors shortlisted for a Richard and Judy book award in 2005 are hoping for a boost in sales following the success of this year's winner.\n",
      "\n",
      "The TV couple's interest in the book world coined the term \"the Richard & Judy effect\" and created the top two best-selling paperbacks of 2004 so far. The finalists for 2005 include Andrew Taylor's The American Boy and Robbie Williams' autobiography Feel. This year's winner, Alice Sebold's The Lovely Bones, sold over one million. Joseph O'Connor's Star of the Sea came second and saw sales increase by 350%. The best read award, on Richard Madeley and Judy Finnigan's Channel 4 show, is part of the British Book Awards. David Mitchell's Booker-shortlisted novel, Cloud Atlas, makes it into this year's top 10 along with several lesser known works.\n",
      "\n",
      "\"There's no doubt that this year's selection of book club entries is the best yet. If anything, the choice is even wider than last time,\" said Madeley. \"It was very hard to follow last year's extremely successful list, but we think this year's books will do even better,\" said Richard and Judy executive producer Amanda Ross. \"We were spoiled for choice and it was tough getting down to only 10 from the 301 submitted.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text1.4.txt', 'r') as file:\n",
    "    # Read the entire contents of the file\n",
    "    contents = file.read()\n",
    "\n",
    "# Print the contents of the file\n",
    "print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0308331-2ccf-4130-8b11-657cc15438c0",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OPEN FILE AND SPLIT INTO SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade8ffa1-46d7-460b-b8de-f4be722ba8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TV couple's interest in the book world coined the term \"the Richard & Judy effect\" and created the top two best-selling paperbacks of 2004 so far\n",
      "The finalists for 2005 include Andrew Taylor's The American Boy and Robbie Williams' autobiography Feel\n",
      "This year's winner, Alice Sebold's The Lovely Bones, sold over one million\n",
      "Joseph O'Connor's Star of the Sea came second and saw sales increase by 350%\n",
      "The best read award, on Richard Madeley and Judy Finnigan's Channel 4 show, is part of the British Book Awards\n",
      "David Mitchell's Booker-shortlisted novel, Cloud Atlas, makes it into this year's top 10 along with several lesser known works.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Open the file \"Text1.txt\" in read mode\n",
    "file = open(\"text1.4.txt\", \"r\")\n",
    "\n",
    "# Read all lines of the file into a list\n",
    "filedata = file.readlines()\n",
    "\n",
    "# Split the first paragraph into sentences based on the period followed by a space\n",
    "article = filedata[4].split(\". \")  # Assuming each sentence ends with \". \".\n",
    "\n",
    "# Initialize an empty list to store individual sentences\n",
    "sentences = []\n",
    "\n",
    "# Iterate through each sentence in the article\n",
    "for sentence in article:\n",
    "    # Print each sentence\n",
    "    print(sentence)\n",
    "    # Replace non-alphabetic characters with spaces and split the sentence into words\n",
    "    sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19879fff-f716-4f88-898f-e3f6a5f133c4",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>OUR DATA: A LIST OF SENTENCES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4512bd3c-257b-4115-b8c9-153fe43e652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences are  [['The', 'TV', \"couple's\", 'interest', 'in', 'the', 'book', 'world', 'coined', 'the', 'term', '\"the', 'Richard', '&', 'Judy', 'effect\"', 'and', 'created', 'the', 'top', 'two', 'best-selling', 'paperbacks', 'of', '2004', 'so', 'far'], ['The', 'finalists', 'for', '2005', 'include', 'Andrew', \"Taylor's\", 'The', 'American', 'Boy', 'and', 'Robbie', \"Williams'\", 'autobiography', 'Feel'], ['This', \"year's\", 'winner,', 'Alice', \"Sebold's\", 'The', 'Lovely', 'Bones,', 'sold', 'over', 'one', 'million'], ['Joseph', \"O'Connor's\", 'Star', 'of', 'the', 'Sea', 'came', 'second', 'and', 'saw', 'sales', 'increase', 'by', '350%'], ['The', 'best', 'read', 'award,', 'on', 'Richard', 'Madeley', 'and', 'Judy', \"Finnigan's\", 'Channel', '4', 'show,', 'is', 'part', 'of', 'the', 'British', 'Book', 'Awards'], ['David', \"Mitchell's\", 'Booker-shortlisted', 'novel,', 'Cloud', 'Atlas,', 'makes', 'it', 'into', 'this', \"year's\", 'top', '10', 'along', 'with', 'several', 'lesser', 'known', 'works.\\n']]\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentences are \", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e0e586-a4a1-43ef-aac4-74bf4ea8d703",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>FUNCTION TO CALCULATE SIMILARITY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ded1f050-7a6b-4643-bb57-0a4a46aeb722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    # Convert all words to lowercase\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "\n",
    "    # Get all unique words from both sentences\n",
    "    all_words = list(set(sent1 + sent2))\n",
    "\n",
    "    # Initialize vectors for both sentences with zeros\n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "\n",
    "    # Build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector1[all_words.index(w)] += 1\n",
    "\n",
    "    # Build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        # Increment the count of the word in the vector\n",
    "        vector2[all_words.index(w)] += 1\n",
    "        return 1 - cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b3f80-4e80-4ce0-9b44-b62ab3d28af4",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>CREATING THE SIMILARITY MATRIX</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96b23237-208b-424e-a85d-ec3943aa078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smilarity matrix \n",
      " [[0.         0.64051262 0.         0.         0.64051262 0.        ]\n",
      " [0.48507125 0.         0.         0.         0.48507125 0.        ]\n",
      " [0.28867513 0.28867513 0.         0.         0.28867513 0.        ]\n",
      " [0.26726124 0.26726124 0.         0.         0.26726124 0.        ]\n",
      " [0.42640143 0.42640143 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.22941573 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    " \n",
    "for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "             if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "             similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "\n",
    "print(\"Smilarity matrix \\n\", similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3598cce4-14bb-450d-b451-90280c6a32eb",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>GET THE PAGERANK SCORES</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c10fbf9-ba5d-4bf9-8f3c-ff54d6e2cac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PageRank scores: {0: 0.21461275182395878, 1: 0.21461275182395878, 2: 0.18155970883347622, 3: 0.12483851367583565, 4: 0.2070558755399856, 5: 0.057320398302784854}\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create a graph from the similarity matrix using NetworkX\n",
    "sentence_similarity_graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "# Calculate the PageRank scores for each node (sentence) in the graph\n",
    "scores = nx.pagerank(sentence_similarity_graph)\n",
    "\n",
    "# Print the PageRank scores\n",
    "print(\"PageRank scores:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cf4b90-80d1-4a6a-ab6d-edb78b87a3cb",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>SORT SENTENCES BY PAGERANK</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "035fe001-d8ae-4e2e-821c-35d62e6ad248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes of top ranked sentences in descending order:\n",
      " [(0.21461275182395878, ['The', 'finalists', 'for', '2005', 'include', 'Andrew', \"Taylor's\", 'The', 'American', 'Boy', 'and', 'Robbie', \"Williams'\", 'autobiography', 'Feel']), (0.21461275182395878, ['The', 'TV', \"couple's\", 'interest', 'in', 'the', 'book', 'world', 'coined', 'the', 'term', '\"the', 'Richard', '&', 'Judy', 'effect\"', 'and', 'created', 'the', 'top', 'two', 'best-selling', 'paperbacks', 'of', '2004', 'so', 'far']), (0.2070558755399856, ['The', 'best', 'read', 'award,', 'on', 'Richard', 'Madeley', 'and', 'Judy', \"Finnigan's\", 'Channel', '4', 'show,', 'is', 'part', 'of', 'the', 'British', 'Book', 'Awards']), (0.18155970883347622, ['This', \"year's\", 'winner,', 'Alice', \"Sebold's\", 'The', 'Lovely', 'Bones,', 'sold', 'over', 'one', 'million']), (0.12483851367583565, ['Joseph', \"O'Connor's\", 'Star', 'of', 'the', 'Sea', 'came', 'second', 'and', 'saw', 'sales', 'increase', 'by', '350%']), (0.057320398302784854, ['David', \"Mitchell's\", 'Booker-shortlisted', 'novel,', 'Cloud', 'Atlas,', 'makes', 'it', 'into', 'this', \"year's\", 'top', '10', 'along', 'with', 'several', 'lesser', 'known', 'works.\\n'])]\n"
     ]
    }
   ],
   "source": [
    "# Sort the sentences based on their PageRank scores in descending order\n",
    "ranked_sentence = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "# Print the indexes of top-ranked sentences in the order of their importance\n",
    "print(\"Indexes of top ranked sentences in descending order:\\n\", ranked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e969c3a-1018-4be9-b3fb-be30a02c92f2",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PICK THE TOP \"N\" SENTENCES </b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72d5c769-3a15-4075-ae92-e7a3e87f198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "How many sentences do you want in the summary?  3\n"
     ]
    }
   ],
   "source": [
    "# Step 5 - How many sentences to pick\n",
    "\n",
    "# Prompt the user to input the number of sentences they want in the summary\n",
    "n = int(input(\"How many sentences do you want in the summary? \"))\n",
    "\n",
    "# Initialize an empty list to store the summarized sentences\n",
    "summarize_text = []\n",
    "\n",
    "# Iterate through the top n ranked sentences and join them into a single string\n",
    "for i in range(n):\n",
    "    # Append the joined sentence to the summarize_text list\n",
    "    summarize_text.append(\" \".join(ranked_sentence[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cbaa82-2ec7-4b2e-a5b3-2c5207eaeac3",
   "metadata": {},
   "source": [
    "<font size=\"3\"><b>PRINTING THE SUMMARY</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "248a2d6c-f6dd-4df4-8771-5400074e3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      " The finalists for 2005 include Andrew Taylor's The American Boy and Robbie Williams' autobiography Feel. The TV couple's interest in the book world coined the term \"the Richard & Judy effect\" and created the top two best-selling paperbacks of 2004 so far. The best read award, on Richard Madeley and Judy Finnigan's Channel 4 show, is part of the British Book Awards\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - Output the summarized text\n",
    "\n",
    "# Print the summarized text by joining the sentences with a period and a space\n",
    "print(\"Summarized Text:\\n\", \". \".join(summarize_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
